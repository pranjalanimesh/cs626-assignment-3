{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import string\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "import nltk\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK resources.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK resources...\")\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        nltk.download('punkt')\n",
    "\n",
    "setup_nltk()\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def get_pos_tags(sentence: str) -> List[str]:\n",
    "    \"\"\"Get POS tags for a sentence using NLTK.\"\"\"\n",
    "    if isinstance(sentence, list):\n",
    "        tokens = sentence\n",
    "    else:\n",
    "        tokens = word_tokenize(sentence)\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    return [tag for _, tag in pos_tagged]\n",
    "\n",
    "def build_pos_tag_dict(dataset) -> Dict[str, int]:\n",
    "    \"\"\"Pre-compute POS tag dictionary from the entire dataset.\"\"\"\n",
    "    pos_tag_dict = {}\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        for instance in dataset[split]:\n",
    "            pos_tags = get_pos_tags(instance['tokens'])\n",
    "            for tag in pos_tags:\n",
    "                if tag not in pos_tag_dict:\n",
    "                    pos_tag_dict[tag] = len(pos_tag_dict)\n",
    "    return pos_tag_dict\n",
    "\n",
    "def load_and_preprocess_data(dataset_name):\n",
    "    \"\"\"Load and preprocess the CoNLL-2003 dataset.\"\"\"\n",
    "    dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "    \n",
    "    X_all = Dataset.from_dict(\n",
    "        {   \n",
    "            'id': dataset['train']['id'] + dataset['test']['id'],\n",
    "            'tokens': dataset['train']['tokens'] + dataset['test']['tokens'],\n",
    "            'pos_tags': dataset['train']['pos_tags'] + dataset['test']['pos_tags'],\n",
    "            'chunk_tags': dataset['train']['chunk_tags'] + dataset['test']['chunk_tags'],\n",
    "            'ner_tags': dataset['train']['ner_tags'] + dataset['test']['ner_tags'],\n",
    "        }\n",
    "    )\n",
    "    dataset['all'] = X_all \n",
    "    return dataset\n",
    "\n",
    "def convert_to_binary_labels(ner_tags: List[int]) -> List[int]:\n",
    "    \"\"\"Convert CoNLL-2003 NER tags to binary labels. 1 is for named entities, 0 is for non-named-entities.\"\"\"\n",
    "    return [1 if tag != 0 else 0 for tag in ner_tags]\n",
    "\n",
    "def extract_features_for_token(token: str, tokens: List[str], pos_tags: List[str], \n",
    "                             pos_tag_dict: Dict[str, int], index: int, \n",
    "                             window_size: int = 2) -> np.ndarray:\n",
    "    \"\"\"Extract features for a single token.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # 1. Case-based features (5 features)\n",
    "    features.extend([\n",
    "        1.0 if token.isupper() else 0.0,\n",
    "        1.0 if token.istitle() else 0.0,\n",
    "        1.0 if token.islower() else 0.0,\n",
    "        1.0 if any(c.isdigit() for c in token) else 0.0,\n",
    "        1.0 if re.match(r'.*[A-Z].*[A-Z].*', token) else 0.0  # 2 uppercase letters\n",
    "    ])\n",
    "    \n",
    "    # 2. Token length feature (1 feature)\n",
    "    features.append(len(token) / 20.0)\n",
    "    \n",
    "    # 3. Position-based features (2 features)\n",
    "    features.extend([\n",
    "        1.0 if index == 0 else 0.0,\n",
    "        1.0 if index == len(tokens) - 1 else 0.0\n",
    "    ])\n",
    "    \n",
    "    # 4. Character-based features (4 features)\n",
    "    features.extend([\n",
    "        1.0 if '.' in token else 0.0,\n",
    "        1.0 if ',' in token else 0.0,\n",
    "        1.0 if '-' in token else 0.0,\n",
    "        1.0 if \"'\" in token else 0.0,\n",
    "        1.0 if \"''\" in token else 0.0,\n",
    "        1.0 if \"(\" in token else 0.0,\n",
    "        1.0 if \")\" in token else 0.0,\n",
    "        1.0 if any(c in string.punctuation for c in token) else 0.0\n",
    "    ])\n",
    "    \n",
    "    # 5. POS tag feature (one-hot encoded)\n",
    "    pos_features = np.zeros(len(pos_tag_dict))\n",
    "    if pos_tags[index] in pos_tag_dict:\n",
    "        pos_features[pos_tag_dict[pos_tags[index]]] = 1.0\n",
    "    features.extend(pos_features)\n",
    "    \n",
    "    # 6. Context window features\n",
    "    for i in range(-window_size, window_size + 1):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        pos = index + i\n",
    "        if pos < 0 or pos >= len(tokens):\n",
    "            # Padding features for out-of-bounds positions (5 features per position)\n",
    "            features.extend([0.0] * 5)\n",
    "        else:\n",
    "            context_token = tokens[pos]\n",
    "            features.extend([\n",
    "                1.0 if context_token.isupper() else 0.0,\n",
    "                1.0 if context_token.istitle() else 0.0,\n",
    "                1.0 if context_token.islower() else 0.0,\n",
    "                len(context_token) / 20.0,\n",
    "                1.0 if pos_tags[pos] in pos_tag_dict else 0.0\n",
    "            ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_features_for_sentence(tokens: List[str], pos_tag_dict: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"Extract features for all tokens in a sentence.\"\"\"\n",
    "    pos_tags = get_pos_tags(tokens)\n",
    "    return np.array([extract_features_for_token(tokens[i], tokens, pos_tags, pos_tag_dict, i) \n",
    "                    for i in range(len(tokens))])\n",
    "\n",
    "def prepare_dataset(dataset_split, pos_tag_dict: Dict[str, int]):\n",
    "    \"\"\"Prepare features and labels for a dataset split.\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for instance in dataset_split:\n",
    "        sentence_features = extract_features_for_sentence(instance['tokens'], pos_tag_dict)\n",
    "        all_features.extend(sentence_features)\n",
    "        all_labels.extend(convert_to_binary_labels(instance['ner_tags']))\n",
    "    \n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "def train_and_evaluate():\n",
    "    \"\"\"Train SVM model and evaluate its performance.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_and_preprocess_data(\"eriktks/conll2003\")\n",
    "\n",
    "    # Build POS tag dictionary from entire dataset\n",
    "    print(\"Building POS tag dictionary...\")\n",
    "    pos_tag_dict = build_pos_tag_dict(dataset)\n",
    "    print(f\"Number of unique POS tags: {len(pos_tag_dict)}\")\n",
    "    \n",
    "    # Prepare train and test sets\n",
    "    print(\"Preparing training data...\")\n",
    "    X_train, y_train = prepare_dataset(dataset['train'], pos_tag_dict)\n",
    "    print(\"Preparing test data...\")\n",
    "    X_test, y_test = prepare_dataset(dataset['all'], pos_tag_dict)\n",
    "    \n",
    "    # Scale features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train SVM\n",
    "    print(\"Training SVM model...\")\n",
    "    svm = LinearSVC(random_state=42, max_iter=2000)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, \n",
    "                                                             average='binary')\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'model': svm,\n",
    "        'scaler': scaler,\n",
    "        'pos_tag_dict': pos_tag_dict\n",
    "    }\n",
    "\n",
    "def predict_sentence(sentence: str, svm_model, scaler, pos_tag_dict) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Predict NER tags for a new sentence.\"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features_for_sentence(tokens, pos_tag_dict)\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_features = scaler.transform(features)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = svm_model.predict(scaled_features)\n",
    "    \n",
    "    return list(zip(tokens, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building POS tag dictionary...\n",
      "Number of unique POS tags: 43\n",
      "Preparing training data...\n",
      "Preparing test data...\n",
      "Scaling features...\n",
      "Training SVM model...\n",
      "Evaluating model...\n"
     ]
    }
   ],
   "source": [
    "results = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.85\n",
      "Recall: 0.94\n",
      "F1 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {results['precision']:.2f}\")\n",
    "print(f\"Recall: {results['recall']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Washington DC is the capital of United States of America\n",
      "Output: Washington_1 DC_1 is_0 the_0 capital_0 of_0 United_1 States_1 of_0 America_1\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Washington DC is the capital of United States of America\"\n",
    "predictions = predict_sentence(\n",
    "    test_sentence, \n",
    "    results['model'], \n",
    "    results['scaler'], \n",
    "    results['pos_tag_dict']\n",
    ")\n",
    "    \n",
    "formatted_output = \" \".join([f\"{token}_{pred}\" for token, pred in predictions])\n",
    "print(f\"\\nInput: {test_sentence}\")\n",
    "print(f\"Output: {formatted_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
